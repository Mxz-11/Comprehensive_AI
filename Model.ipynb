{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Downloading diffusers-0.32.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (7.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (0.26.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (0.4.5)\n",
      "Requirement already satisfied: Pillow in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from diffusers) (10.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.2->diffusers) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from requests->diffusers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from requests->diffusers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from requests->diffusers) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxva\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.2->diffusers) (0.4.6)\n",
      "Downloading diffusers-0.32.1-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.6/3.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: diffusers\n",
      "Successfully installed diffusers-0.32.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers huggingface_hub accelerate  gradio bitsandbytes timm sentencepiece diffusers\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login;\n",
    "from transformers import pipeline, AutoImageProcessor, AutoModelForObjectDetection, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan;\n",
    "from accelerate import init_empty_weights;\n",
    "import torch;\n",
    "from PIL import Image, ImageDraw;\n",
    "import gradio as gr;\n",
    "import soundfile as sf;\n",
    "import os;\n",
    "from diffusers import StableDiffusionPipeline;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6fb852917a4712a5dc419cf873962d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2767b941ef4661bfeeec886d31b245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e184a865010d4be58186eaed365494d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_HOME\"] = \"./bot\";\n",
    "\n",
    "device = \"cpu\";\n",
    "if torch.cuda.is_available ():\n",
    "    device = torch.device (\"cuda\");\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device (\"mps\");\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\";\n",
    "tokenizer = AutoTokenizer.from_pretrained (model_id);\n",
    "model = AutoModelForCausalLM.from_pretrained (\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None\n",
    ").to (device);\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained (\"microsoft/speecht5_tts\");\n",
    "tts_model = SpeechT5ForTextToSpeech.from_pretrained (\"microsoft/speecht5_tts\");\n",
    "vocoder = SpeechT5HifiGan.from_pretrained (\"microsoft/speecht5_hifigan\");\n",
    "speaker_embeddings = torch.randn (1, 512);\n",
    "\n",
    "ckpt = \"yainage90/fashion-object-detection\";\n",
    "image_processor = AutoImageProcessor.from_pretrained (ckpt);\n",
    "object_model = AutoModelForObjectDetection.from_pretrained (ckpt).to (device);\n",
    "\n",
    "stable_diffusion_model_id = \"runwayml/stable-diffusion-v1-5\";\n",
    "pipe = StableDiffusionPipeline.from_pretrained (stable_diffusion_model_id, torch_dtype = torch.float32);\n",
    "\n",
    "global_context = \"\";\n",
    "\n",
    "def generate_text_and_audio (prompt):\n",
    "    global global_context;\n",
    "\n",
    "    global_context += f\"User: {prompt}\\n\";\n",
    "    formatted_prompt = f\"{global_context}Assistant:\";\n",
    "\n",
    "    inputs = tokenizer (formatted_prompt, return_tensors=\"pt\").to (device);\n",
    "    generate_ids = model.generate (\n",
    "        inputs.input_ids,\n",
    "        temperature = 0.1,\n",
    "        top_p = 0.85,\n",
    "        top_k = 50,\n",
    "        repetition_penalty = 1.2,\n",
    "        do_sample = True,\n",
    "        num_return_sequences = 1,\n",
    "        max_length = 200\n",
    "    )\n",
    "    result_text = tokenizer.decode (generate_ids[0], skip_special_tokens = True).strip ();\n",
    "    global_context += f\"Assistant: {result_text}\\n\";\n",
    "\n",
    "    inputs = processor (text = result_text, return_tensors = \"pt\");\n",
    "    speech = tts_model.generate_speech (inputs[\"input_ids\"], speaker_embeddings, vocoder = vocoder);\n",
    "\n",
    "    audio_path = \"response_audio.wav\";\n",
    "    sf.write (audio_path, speech.cpu ().numpy (), samplerate = 16000);\n",
    "\n",
    "    return result_text, audio_path;\n",
    "\n",
    "def detect_objects (image):\n",
    "\n",
    "    image = image.convert (\"RGB\");\n",
    "\n",
    "    with torch.no_grad ():\n",
    "        inputs = image_processor (images = [image], return_tensors = \"pt\").to (device);\n",
    "        outputs = object_model (**inputs);\n",
    "        target_sizes = torch.tensor ([[image.size[1], image.size[0]]]).to (device);\n",
    "        results = image_processor.post_process_object_detection (outputs, threshold = 0.4, target_sizes = target_sizes)[0];\n",
    "\n",
    "    draw = ImageDraw.Draw (image);\n",
    "    for score, label, box in zip (results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        score = round (score.item(), 2);\n",
    "        label = object_model.config.id2label[label.item ()];\n",
    "        box = [round (i.item(), 2) for i in box];\n",
    "        draw.rectangle (box, outline = \"red\", width = 3);\n",
    "        draw.text ((box[0], box[1] - 10), f\"{label} ({score})\", fill = \"red\");\n",
    "\n",
    "    return image;\n",
    "\n",
    "def generate_image (prompt, height, width):\n",
    "    image = pipe (prompt, height = int (height), width = int (width)).images[0];\n",
    "    return image;\n",
    "\n",
    "def build_interface ():\n",
    "    with gr.Blocks () as demo:\n",
    "        gr.Markdown (\"# Text generator + TTS, object detection and image generator\");\n",
    "\n",
    "        with gr.Tab (\"Text generator + TTS\"):\n",
    "            gr.Markdown (\"Introduce your response\");\n",
    "            with gr.Row ():\n",
    "                prompt = gr.Textbox (label = \"Ask\", placeholder = \"Introduce your question...\");\n",
    "                generate_button = gr.Button (\"Generate\");\n",
    "            with gr.Row ():\n",
    "                output_text = gr.Textbox (label = \"Generated response\", lines = 10);\n",
    "                output_audio = gr.Audio (label = \"Answer audio\");\n",
    "            generate_button.click (\n",
    "                generate_text_and_audio,\n",
    "                inputs = [prompt],\n",
    "                outputs = [output_text, output_audio]\n",
    "            );\n",
    "\n",
    "        with gr.Tab (\"Object detection\"):\n",
    "            gr.Markdown (\"Upload an image\");\n",
    "            with gr.Row ():\n",
    "                image_input = gr.Image (type = \"pil\", label = \"Upload your image\");\n",
    "                detect_button = gr.Button (\"Object detection\");\n",
    "            with gr.Row ():\n",
    "                image_output = gr.Image (type = \"pil\", label = \"Result\");\n",
    "            detect_button.click (\n",
    "                detect_objects,\n",
    "                inputs = image_input,\n",
    "                outputs = image_output\n",
    "            );\n",
    "\n",
    "        with gr.Tab (\"Image generator\"):\n",
    "            gr.Markdown (\"Generate an image from a prompt\");\n",
    "            with gr.Row ():\n",
    "                image_prompt = gr.Textbox (label = \"Text for the image\", placeholder = \"Describe the image to generate\");\n",
    "                image_height = gr.Number (label = \"Height\", value = 512);\n",
    "                image_width = gr.Number (label = \"Width\", value = 512);\n",
    "                generate_image_button = gr.Button (\"Generate image\");\n",
    "            with gr.Row ():\n",
    "                generated_image = gr.Image (label = \"Generated image\");\n",
    "            generate_image_button.click (\n",
    "                generate_image,\n",
    "                inputs = [image_prompt, image_height, image_width],\n",
    "                outputs = generated_image\n",
    "            );\n",
    "\n",
    "    return demo;\n",
    "\n",
    "demo = build_interface ();\n",
    "demo.launch ();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
